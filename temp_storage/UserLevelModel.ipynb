{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transaction Level Models\n",
    "\n",
    "While it was expected that aggregation of features in a somewhat arbitrary manner would result in the loss of information, it was not expected that such an approach would not be able to beat the baseline score of guessing only zeros.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "import catboost as cb\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sqlalchemy.create_engine('postgresql://romandtse:duckthewut@localhost:5432/training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Queries\n",
    "\n",
    "We format columns to insert into our query in this section.  These will include:\n",
    "- Sum of the visit numbers (inspired by previous analysis)\n",
    "- Sum of page views, assumed the more intuitive alternative over hits\n",
    "- Bounce rate, at least to rule out those with a bounce rate of 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try brute forcing our way through modeling with user level aggregation features.  First, we remind ourselves the types involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/field_dict.pkl', 'rb') as f:\n",
    "    field_dict = pickle.load(f)\n",
    "    \n",
    "with open('../pickles/useless_fields.pkl', 'rb') as f:\n",
    "    useless_fields = pickle.load(f)\n",
    "\n",
    "with open('../pickles/adwordsClickInfo_keys.pkl', 'rb') as f:\n",
    "    adwordsClickInfo_keys = pickle.load(f)\n",
    "    \n",
    "with open('../pickles/channel_groups.pkl', 'rb') as f:\n",
    "    channel_groups = pickle.load(f)\n",
    "    \n",
    "with open('../pickles/field_vals.pkl', 'rb') as f:\n",
    "    field_vals = pickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "query = \"\"\"\n",
    "SELECT fullVisitorId\n",
    "FROM train_data\n",
    "GROUP BY fullVisitorId\n",
    "HAVING SUM(CAST(totals ->> 'transactionRevenue' AS numeric)) > 0\n",
    "\"\"\"\n",
    "\n",
    "customers = pd.read_sql_query(query, engine)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('../pickles/train_customer_ids.pkl', 'wb') as f:\n",
    "    pickle.dump(customers, f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "query = \"\"\"\n",
    "SELECT fullVisitorId\n",
    "FROM train_data\n",
    "GROUP BY fullVisitorId\n",
    "HAVING SUM(CAST(totals ->> 'transactionRevenue' AS numeric)) IS NULL\n",
    "\"\"\"\n",
    "\n",
    "lookers = pd.read_sql_query(query, engine)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('./pickles/train_looker_ids.pkl', 'wb') as f:\n",
    "    pickle.dump(lookers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/train_customer_ids.pkl', 'rb') as f:\n",
    "    train_customer_ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revTemplate(key, name, num_type = 'FLOAT'):\n",
    "    return f\"\"\"COALESCE(CAST({key} ->> '{name}' AS {num_type}), 0)/10^6 AS {name}\"\"\"\n",
    "\n",
    "def jnumTemplate(key, name, num_type = 'INT'):\n",
    "    return f\"\"\"COALESCE(CAST({key} ->> '{name}' AS {num_type}), 0) AS {name}\"\"\"\n",
    "\n",
    "def numTemplate(name):\n",
    "    return f\"\"\"COALESCE({name}, 0) AS {name}\"\"\"\n",
    "\n",
    "def jstrTemplate(key, name):\n",
    "    return f\"{key} ->> '{name}' AS {name}\"\n",
    "\n",
    "def strTemplate(name):\n",
    "    return f\"{name}\"\n",
    "\n",
    "def adwordsTemplate(name):\n",
    "    return f\"CAST(trafficSource ->> 'adwordsClickInfo' AS JSONB) ->> '{name}' AS {name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQuery(dataset = 'train'):\n",
    "    numeric_cols = ['visitNumber', 'newVisits', 'bounces', 'pageviews', 'visits', 'hits', 'transactionRevenue']\n",
    "    if dataset != 'train':\n",
    "        numeric_cols.pop()\n",
    "    selects = []\n",
    "    for cat, subcats in field_dict.items():\n",
    "        for subcat in subcats:\n",
    "            if subcat not in useless_fields[dataset]:\n",
    "                if subcat == 'transactionRevenue':\n",
    "                    selects.append(revTemplate(cat, subcat, 'NUMERIC'))\n",
    "                elif subcat in numeric_cols:\n",
    "                    selects.append(jnumTemplate(cat, subcat))\n",
    "                elif subcat == 'adwordsClickInfo':\n",
    "                    for key in adwordsClickInfo_keys:\n",
    "                        selects.append(adwordsTemplate(key))\n",
    "                else:\n",
    "                    selects.append(jstrTemplate(cat, subcat))\n",
    "    selects.extend([numTemplate('visitNumber'), \n",
    "                    strTemplate('channelGrouping'),\n",
    "                    strTemplate('fullVisitorId'),\n",
    "                    numTemplate('visitStartTime'),\n",
    "                   ])\n",
    "    return ', '.join(selects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstring = getQuery()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/top_order.pkl', 'rb') as f:\n",
    "    top_order = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create user_list by\n",
    "def getUserData(user_list):\n",
    "    users = \"\\', \\'\".join(user_list)\n",
    "    query = f\"\"\"\n",
    "    SELECT {qstring}\n",
    "    FROM train_data\n",
    "    WHERE fullVisitorId IN (\\'{users}\\')\n",
    "    \"\"\"\n",
    "\n",
    "    return pd.read_sql_query(query, engine, parse_dates=['visitstarttime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/objects.pkl', 'rb') as f:\n",
    "    objects = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tablet'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_vals['train']['device']['deviceCategory'][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By introducing all possible values of fields ahead of time for dummying, independent of whether they show up in the training set or not, we fail to simulate the fact that we have no idea whether we have captured all the features.  The categories included here, though are fairly set in stone; there probably are not many sub continents that have yet to appear in the store's history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86400"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "60*60*24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjustCols(df, dataset='train'):\n",
    "    back_looking = ['bounces', 'hits', 'newvisits', 'pageviews']\n",
    "    \n",
    "    df = df.sort_values(['fullvisitorid','visitstarttime'])\n",
    "    #turns out the for loop checks col_order dynamically, temporary list needed to avoid infinite loop\n",
    "    for col in back_looking:\n",
    "        df[f'{col}last'] = df.groupby('fullvisitorid')[col].shift(1)\n",
    "        df[f'{col}two'] = df.groupby('fullvisitorid')[col].shift(2)\n",
    "    df['sincelast'] = df.groupby('fullvisitorid').visitstarttime.diff().map(lambda x: x.days + x.seconds/86400)\n",
    "    df['sincetwo'] = df.groupby('fullvisitorid').visitstarttime.diff(2).map(lambda x: x.days + x.seconds/86400)\n",
    "    df['hour'] = df.visitstarttime.map(lambda x: x.hour)\n",
    "    df['weekday'] = df.visitstarttime.map(lambda x: x.dayofweek)\n",
    "    \n",
    "    return df.drop(['fullvisitorid', 'visitstarttime'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/train_customer_ids.pkl', 'rb') as f:\n",
    "    train_customer_ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/train_looker_ids.pkl', 'rb') as f:\n",
    "    train_looker_ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle now so we can just iterate through lists\n",
    "from random import shuffle\n",
    "\n",
    "def stratifiedIdSplit(test_size=0.5):\n",
    "    customer_size = int(len(train_customer_ids)*test_size)\n",
    "    looker_size = int(len(train_looker_ids)*test_size)\n",
    "    \n",
    "    test_customers = list(np.random.choice(train_customer_ids.T.values[0], replace=False, size=customer_size))\n",
    "    test_lookers = list(np.random.choice(train_looker_ids.T.values[0], replace=False, size=looker_size))\n",
    "    \n",
    "    train_customers = list(set(train_customer_ids.T.values[0]).difference(set(test_customers)))\n",
    "    train_lookers = list(set(train_looker_ids.T.values[0]).difference(set(test_lookers)))\n",
    "    \n",
    "    test_customers.extend(test_lookers)\n",
    "    train_customers.extend(train_lookers)\n",
    "    \n",
    "    shuffle(test_customers)\n",
    "    shuffle(train_customers)\n",
    "    \n",
    "    return train_customers, test_customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, test_ids = stratifiedIdSplit(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train_ids.remove(top_order.iloc[0,0])\n",
    "except:\n",
    "    test_ids.remove(top_order.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['devicecategory', 'ismobile', 'browser', 'operatingsystem', 'city',\n",
       "       'continent', 'country', 'metro', 'networkdomain', 'region',\n",
       "       'subcontinent', 'bounces', 'hits', 'newvisits', 'pageviews',\n",
       "       'transactionrevenue', 'adcontent', 'adnetworktype',\n",
       "       'criteriaparameters', 'gclid', 'isvideoad', 'page', 'slot',\n",
       "       'targetingcriteria', 'campaign', 'campaigncode', 'istruedirect',\n",
       "       'keyword', 'medium', 'referralpath', 'source', 'visitnumber',\n",
       "       'channelgrouping', 'visitstarttime'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getUserData([test_ids[0]]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createChunk(ids_list, size=10000):\n",
    "    new_size = len(ids_list)\n",
    "    if  new_size > size:\n",
    "        new_size = size\n",
    "    someppl = ids_list\n",
    "    shuffle(someppl)\n",
    "    someppl = someppl[:new_size]\n",
    "    chunk = getUserData(someppl)\n",
    "    chunk = adjustCols(chunk)\n",
    "    \n",
    "    return chunk.fillna(0), ids_list[new_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_df = adjustCols(getUserData([top_order.iloc[0][0]])).drop('transactionrevenue', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_vals = adjustCols(getUserData([top_order.iloc[0][0]])).transactionrevenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feets = np.where(trial_df.dtypes == object)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['devicecategory', 'ismobile', 'browser', 'operatingsystem', 'city',\n",
       "       'continent', 'country', 'metro', 'networkdomain', 'region',\n",
       "       'subcontinent', 'adcontent', 'adnetworktype', 'criteriaparameters',\n",
       "       'gclid', 'isvideoad', 'page', 'slot', 'targetingcriteria', 'campaign',\n",
       "       'campaigncode', 'istruedirect', 'keyword', 'medium', 'referralpath',\n",
       "       'source', 'channelgrouping'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_df.columns[cat_feets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of training a set of trees and losing most of the trees, let's make an ensemble.  We can blend it and fit to the actual target at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric iscalculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 48.5402163\ttest: 18.1288499\tbest: 18.1288499 (0)\ttotal: 1.53s\tremaining: 12m 45s\n",
      "50:\tlearn: 48.0358898\ttest: 17.4573471\tbest: 17.4573471 (50)\ttotal: 50.1s\tremaining: 7m 20s\n",
      "100:\tlearn: 47.7134432\ttest: 17.1657098\tbest: 17.1657098 (100)\ttotal: 1m 46s\tremaining: 7m\n",
      "150:\tlearn: 47.4278259\ttest: 16.9622723\tbest: 16.9622723 (150)\ttotal: 2m 40s\tremaining: 6m 10s\n",
      "200:\tlearn: 47.2177783\ttest: 16.8770005\tbest: 16.8770005 (200)\ttotal: 3m 35s\tremaining: 5m 19s\n",
      "250:\tlearn: 47.0499854\ttest: 16.8358820\tbest: 16.8358820 (250)\ttotal: 4m 30s\tremaining: 4m 28s\n",
      "300:\tlearn: 46.8798508\ttest: 16.7676452\tbest: 16.7676452 (300)\ttotal: 5m 20s\tremaining: 3m 31s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 16.74211914\n",
      "bestIteration = 330\n",
      "\n",
      "Shrink model to first 331 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric iscalculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 31.4606848\ttest: 36.2361573\tbest: 36.2361573 (0)\ttotal: 1.39s\tremaining: 11m 34s\n",
      "50:\tlearn: 30.7259039\ttest: 35.5534834\tbest: 35.5534834 (50)\ttotal: 54.9s\tremaining: 8m 3s\n",
      "100:\tlearn: 30.3572606\ttest: 35.2246750\tbest: 35.2246750 (100)\ttotal: 1m 53s\tremaining: 7m 27s\n",
      "150:\tlearn: 30.1465167\ttest: 35.0415525\tbest: 35.0415525 (150)\ttotal: 2m 50s\tremaining: 6m 33s\n",
      "200:\tlearn: 30.0112226\ttest: 34.9168558\tbest: 34.9168558 (200)\ttotal: 3m 46s\tremaining: 5m 37s\n",
      "250:\tlearn: 29.9031159\ttest: 34.8341956\tbest: 34.8341956 (250)\ttotal: 4m 43s\tremaining: 4m 41s\n",
      "300:\tlearn: 29.8244846\ttest: 34.7709446\tbest: 34.7709446 (300)\ttotal: 5m 38s\tremaining: 3m 43s\n",
      "350:\tlearn: 29.7616933\ttest: 34.7300594\tbest: 34.7300594 (350)\ttotal: 6m 32s\tremaining: 2m 46s\n",
      "400:\tlearn: 29.7105514\ttest: 34.6933535\tbest: 34.6933535 (400)\ttotal: 7m 28s\tremaining: 1m 50s\n",
      "450:\tlearn: 29.6683074\ttest: 34.6665705\tbest: 34.6665469 (449)\ttotal: 8m 25s\tremaining: 54.9s\n",
      "499:\tlearn: 29.6315791\ttest: 34.6398282\tbest: 34.6398282 (499)\ttotal: 9m 21s\tremaining: 0us\n",
      "\n",
      "bestTest = 34.6398282\n",
      "bestIteration = 499\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric iscalculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 53.6561854\ttest: 31.4136747\tbest: 31.4136747 (0)\ttotal: 647ms\tremaining: 5m 22s\n",
      "50:\tlearn: 53.2021717\ttest: 30.9177321\tbest: 30.9177321 (50)\ttotal: 29.9s\tremaining: 4m 23s\n",
      "100:\tlearn: 52.9469816\ttest: 30.6596638\tbest: 30.6596638 (100)\ttotal: 59.5s\tremaining: 3m 55s\n",
      "150:\tlearn: 52.7502911\ttest: 30.4948680\tbest: 30.4948680 (150)\ttotal: 1m 28s\tremaining: 3m 25s\n",
      "200:\tlearn: 52.6244472\ttest: 30.3985902\tbest: 30.3985902 (200)\ttotal: 1m 58s\tremaining: 2m 56s\n",
      "250:\tlearn: 52.4885832\ttest: 30.2997999\tbest: 30.2997999 (250)\ttotal: 2m 28s\tremaining: 2m 27s\n",
      "300:\tlearn: 52.3829378\ttest: 30.2234138\tbest: 30.2234138 (300)\ttotal: 2m 58s\tremaining: 1m 57s\n",
      "350:\tlearn: 52.2801877\ttest: 30.1486081\tbest: 30.1486081 (350)\ttotal: 3m 23s\tremaining: 1m 26s\n",
      "400:\tlearn: 52.1974426\ttest: 30.0949433\tbest: 30.0949433 (400)\ttotal: 3m 51s\tremaining: 57.2s\n",
      "450:\tlearn: 52.1229274\ttest: 30.0589655\tbest: 30.0589655 (450)\ttotal: 4m 21s\tremaining: 28.4s\n",
      "499:\tlearn: 52.0710153\ttest: 30.0376473\tbest: 30.0374015 (497)\ttotal: 4m 51s\tremaining: 0us\n",
      "\n",
      "bestTest = 30.03740149\n",
      "bestIteration = 497\n",
      "\n",
      "Shrink model to first 498 iterations.\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "id_holder = train_ids\n",
    "\n",
    "i = 0\n",
    "for n in range(3):\n",
    "    models.append(cb.CatBoostRegressor(iterations = 500,\n",
    "                                       learning_rate  = .01,\n",
    "                                       l2_leaf_reg = 100,\n",
    "                                       cat_features = cat_feets,\n",
    "                                       verbose = True))\n",
    "    \n",
    "    eval_chunk = createChunk(test_ids, size=10000)[0]\n",
    "    eval_chunk = cb.Pool(eval_chunk.drop('transactionrevenue', axis=1), \n",
    "                     eval_chunk.transactionrevenue, \n",
    "                     cat_features=cat_feets)\n",
    "    chunk, id_holder = createChunk(id_holder, 200000)\n",
    "    \n",
    "    models[i].fit(X = chunk.drop('transactionrevenue', axis=1),\n",
    "               y = chunk.transactionrevenue,\n",
    "               use_best_model = True,\n",
    "               eval_set = eval_chunk,\n",
    "               early_stopping_rounds = 10,\n",
    "               metric_period = 50)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/cb_stage1.pkl', 'wb') as f:\n",
    "    pickle.dump(models, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('devicecategory', 0.03568502871737648),\n",
       " ('ismobile', 0.0),\n",
       " ('browser', 0.042217954952567555),\n",
       " ('operatingsystem', 0.9084205463950143),\n",
       " ('city', 0.3194187718278321),\n",
       " ('continent', 0.14750723597375096),\n",
       " ('country', 0.3451644477459273),\n",
       " ('metro', 1.9992127540925473),\n",
       " ('networkdomain', 0.0),\n",
       " ('region', 0.28415886238935956),\n",
       " ('subcontinent', 3.696166144267412),\n",
       " ('bounces', 0.0),\n",
       " ('hits', 6.60389409510616),\n",
       " ('newvisits', 0.0),\n",
       " ('pageviews', 40.18660942117891),\n",
       " ('adcontent', 0.048209127012857116),\n",
       " ('adnetworktype', 0.0),\n",
       " ('criteriaparameters', 0.0),\n",
       " ('gclid', 0.0),\n",
       " ('isvideoad', 0.0),\n",
       " ('page', 0.0033935249506453257),\n",
       " ('slot', 0.01518940587194709),\n",
       " ('targetingcriteria', 0.0),\n",
       " ('campaign', 0.02767271014167874),\n",
       " ('campaigncode', 0.0),\n",
       " ('istruedirect', 2.207223638770114),\n",
       " ('keyword', 0.037411567681239995),\n",
       " ('medium', 5.804544398967355),\n",
       " ('referralpath', 0.04540197448352181),\n",
       " ('source', 2.4285421920929795),\n",
       " ('visitnumber', 5.935479069759905),\n",
       " ('channelgrouping', 2.0416480350303403),\n",
       " ('bounceslast', 0.1905557660430665),\n",
       " ('bouncestwo', 1.5693460621073803),\n",
       " ('hitslast', 1.7233919538071634),\n",
       " ('hitstwo', 1.493243405498789),\n",
       " ('newvisitslast', 0.01654153864526794),\n",
       " ('newvisitstwo', 0.1463562764586796),\n",
       " ('pageviewslast', 1.9961407610566413),\n",
       " ('pageviewstwo', 0.8980506706451021),\n",
       " ('sincelast', 2.1941256470552966),\n",
       " ('sincetwo', 10.791426241296278),\n",
       " ('hour', 3.7402909884938342),\n",
       " ('weekday', 2.077359781483041)]"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(trial_df.columns, a.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
