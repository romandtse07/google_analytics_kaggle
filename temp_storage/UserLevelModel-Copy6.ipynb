{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transaction Level Models\n",
    "\n",
    "While it was expected that aggregation of features in a somewhat arbitrary manner would result in the loss of information, it was not expected that such an approach would not be able to beat the baseline score of guessing only zeros.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_9.4.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "import catboost as cb\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sqlalchemy.create_engine('postgresql://romandtse:duckthewut@localhost:5432/training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/field_dict.pkl', 'rb') as f:\n",
    "    field_dict = pickle.load(f)\n",
    "    \n",
    "with open('../pickles/useless_fields.pkl', 'rb') as f:\n",
    "    useless_fields = pickle.load(f)\n",
    "\n",
    "with open('../pickles/adwordsClickInfo_keys.pkl', 'rb') as f:\n",
    "    adwordsClickInfo_keys = pickle.load(f)\n",
    "    \n",
    "with open('../pickles/channel_groups.pkl', 'rb') as f:\n",
    "    channel_groups = pickle.load(f)\n",
    "    \n",
    "with open('../pickles/field_vals.pkl', 'rb') as f:\n",
    "    field_vals = pickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "query = \"\"\"\n",
    "SELECT fullVisitorId\n",
    "FROM train_data\n",
    "GROUP BY fullVisitorId\n",
    "HAVING SUM(CAST(totals ->> 'transactionRevenue' AS numeric)) > 0\n",
    "\"\"\"\n",
    "\n",
    "customers = pd.read_sql_query(query, engine)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('../pickles/train_customer_ids.pkl', 'wb') as f:\n",
    "    pickle.dump(customers, f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "query = \"\"\"\n",
    "SELECT fullVisitorId\n",
    "FROM train_data\n",
    "GROUP BY fullVisitorId\n",
    "HAVING SUM(CAST(totals ->> 'transactionRevenue' AS numeric)) IS NULL\n",
    "\"\"\"\n",
    "\n",
    "lookers = pd.read_sql_query(query, engine)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('./pickles/train_looker_ids.pkl', 'wb') as f:\n",
    "    pickle.dump(lookers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/train_customer_ids.pkl', 'rb') as f:\n",
    "    train_customer_ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is the question of scaling the revenue.  By coincidence, or maybe by design, the target is actually scaled by the natural log.  When we fit a regressor, there will necessarily be negative values.  Since we will send these values through another model, we should consider scaling everything by the natural log here; the argument is not that we pretend a single session should imitate a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revTemplate(key, name, num_type = 'FLOAT'):\n",
    "    return f\"\"\"COALESCE(CAST({key} ->> '{name}' AS {num_type}), 0)/10^6 AS {name}\"\"\"\n",
    "\n",
    "def jnumTemplate(key, name, num_type = 'INT'):\n",
    "    return f\"\"\"COALESCE(CAST({key} ->> '{name}' AS {num_type}), 0) AS {name}\"\"\"\n",
    "\n",
    "def numTemplate(name):\n",
    "    return f\"\"\"COALESCE({name}, 0) AS {name}\"\"\"\n",
    "\n",
    "def jstrTemplate(key, name):\n",
    "    return f\"{key} ->> '{name}' AS {name}\"\n",
    "\n",
    "def strTemplate(name):\n",
    "    return f\"{name}\"\n",
    "\n",
    "def adwordsTemplate(name):\n",
    "    return f\"CAST(trafficSource ->> 'adwordsClickInfo' AS JSONB) ->> '{name}' AS {name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQuery(dataset = 'train'):\n",
    "    numeric_cols = ['visitNumber', 'newVisits', 'bounces', 'pageviews', 'visits', 'hits', 'transactionRevenue']\n",
    "    if dataset != 'train':\n",
    "        numeric_cols.pop()\n",
    "    selects = []\n",
    "    for cat, subcats in field_dict.items():\n",
    "        for subcat in subcats:\n",
    "            if subcat not in useless_fields[dataset]:\n",
    "                if subcat == 'transactionRevenue':\n",
    "                    selects.append(revTemplate(cat, subcat, 'NUMERIC'))\n",
    "                elif subcat in numeric_cols:\n",
    "                    selects.append(jnumTemplate(cat, subcat))\n",
    "                elif subcat == 'adwordsClickInfo':\n",
    "                    for key in adwordsClickInfo_keys:\n",
    "                        selects.append(adwordsTemplate(key))\n",
    "                else:\n",
    "                    selects.append(jstrTemplate(cat, subcat))\n",
    "    selects.extend([numTemplate('visitNumber'), \n",
    "                    strTemplate('channelGrouping'),\n",
    "                    strTemplate('fullVisitorId'),\n",
    "                    numTemplate('visitStartTime'),\n",
    "                   ])\n",
    "    return ', '.join(selects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstring = getQuery()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/top_order.pkl', 'rb') as f:\n",
    "    top_order = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note to self: if you don't change the ids list, you will lose a lot of ids.\n",
    "#you will train on less data than you think, but you won't inflate the score at least by having those you excluded.\n",
    "def getUserData(user_list):\n",
    "    users = \"\\', \\'\".join(user_list)\n",
    "    query = f\"\"\"\n",
    "    SELECT {qstring}\n",
    "    FROM train_data\n",
    "    WHERE fullVisitorId IN (\\'{users}\\')\n",
    "    \"\"\"\n",
    "\n",
    "    return pd.read_sql_query(query, engine, parse_dates=['visitstarttime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/objects.pkl', 'rb') as f:\n",
    "    objects = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By introducing all possible values of fields ahead of time for dummying, independent of whether they show up in the training set or not, we fail to simulate the fact that we have no idea whether we have captured all the features.  The categories included here, though are fairly set in stone; there probably are not many sub continents that have yet to appear in the store's history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjustCols(df, drop_ids = True, ignore_bounce = True):\n",
    "    back_looking = ['bounces', 'hits', 'newvisits', 'pageviews']\n",
    "    \n",
    "    df = df.sort_values(['fullvisitorid','visitstarttime'])\n",
    "    #turns out the for loop checks col_order dynamically, temporary list needed to avoid infinite loop\n",
    "    for col in back_looking:\n",
    "        df[f'{col}last'] = df.groupby('fullvisitorid')[col].shift(1)\n",
    "        df[f'{col}two'] = df.groupby('fullvisitorid')[col].shift(2)\n",
    "    df['sincelast'] = df.groupby('fullvisitorid').visitstarttime.diff().map(lambda x: x.days + x.seconds/86400)\n",
    "    df['sincetwo'] = df.groupby('fullvisitorid').visitstarttime.diff(2).map(lambda x: x.days + x.seconds/86400)\n",
    "    df['hour'] = df.visitstarttime.map(lambda x: x.hour)\n",
    "    df['weekday'] = df.visitstarttime.map(lambda x: x.dayofweek)\n",
    "    df['month'] = df.visitstarttime.map(lambda x: x.month)\n",
    "    \n",
    "    if drop_ids:\n",
    "        df = df.drop('fullvisitorid', axis=1)\n",
    "        \n",
    "    if ignore_bounce:\n",
    "        df = df.query('bounces==0')\n",
    " \n",
    "    return df.drop('visitstarttime', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/train_customer_ids.pkl', 'rb') as f:\n",
    "    train_customer_ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/train_looker_ids.pkl', 'rb') as f:\n",
    "    train_looker_ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle now so we can just iterate through lists\n",
    "from random import shuffle\n",
    "\n",
    "def stratifiedIdSplit(test_size=0.5):\n",
    "    customer_size = int(len(train_customer_ids)*test_size)\n",
    "    looker_size = int(len(train_looker_ids)*test_size)\n",
    "    \n",
    "    test_customers = list(np.random.choice(train_customer_ids.T.values[0], replace=False, size=customer_size))\n",
    "    test_lookers = list(np.random.choice(train_looker_ids.T.values[0], replace=False, size=looker_size))\n",
    "    \n",
    "    train_customers = list(set(train_customer_ids.T.values[0]).difference(set(test_customers)))\n",
    "    train_lookers = list(set(train_looker_ids.T.values[0]).difference(set(test_lookers)))\n",
    "    \n",
    "    test_customers.extend(test_lookers)\n",
    "    train_customers.extend(train_lookers)\n",
    "    \n",
    "    shuffle(test_customers)\n",
    "    shuffle(train_customers)\n",
    "    \n",
    "    return train_customers, test_customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, test_ids = stratifiedIdSplit(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we can hope a machine can learn what is unique about this outlier, the fact is that there is no one else like this user.  In our ensemble later, considering it is a system of gradient boosted trees, there is a very good chance that one of the forests will be awful at guessing because it was fit to minimize the error it would get from this outlier point.  With all its activity, though, perhaps the behavior could still fall in line with the other points of data.  For the fear of overfitting to this point, and because our validation can tell us nothing about how well it does on similar outliers (because there are none like it), we remove it from our model for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train_ids.remove(top_order.iloc[0,0])\n",
    "except:\n",
    "    test_ids.remove(top_order.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['devicecategory', 'ismobile', 'browser', 'operatingsystem', 'city',\n",
       "       'continent', 'country', 'metro', 'networkdomain', 'region',\n",
       "       'subcontinent', 'bounces', 'hits', 'newvisits', 'pageviews',\n",
       "       'transactionrevenue', 'adcontent', 'adnetworktype',\n",
       "       'criteriaparameters', 'gclid', 'isvideoad', 'page', 'slot',\n",
       "       'targetingcriteria', 'campaign', 'campaigncode', 'istruedirect',\n",
       "       'keyword', 'medium', 'referralpath', 'source', 'visitnumber',\n",
       "       'channelgrouping', 'fullvisitorid', 'visitstarttime'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getUserData([test_ids[0]]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createChunk(ids_list, size, drop_ids = True, ignore_bounce=True):\n",
    "    new_size = len(ids_list)\n",
    "    if  new_size > size:\n",
    "        new_size = size\n",
    "    someppl = ids_list\n",
    "    shuffle(someppl)\n",
    "    someppl = someppl[:new_size]\n",
    "    chunk = getUserData(someppl)\n",
    "    chunk = adjustCols(chunk, drop_ids, ignore_bounce)\n",
    "    \n",
    "    return chunk.fillna(0), ids_list[new_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_df = adjustCols(getUserData([top_order.iloc[0][0]])).drop('transactionrevenue', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_vals = adjustCols(getUserData([top_order.iloc[0][0]])).transactionrevenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feets = np.where(trial_df.dtypes == object)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['devicecategory', 'ismobile', 'browser', 'operatingsystem', 'city',\n",
       "       'continent', 'country', 'metro', 'networkdomain', 'region',\n",
       "       'subcontinent', 'adcontent', 'adnetworktype', 'criteriaparameters',\n",
       "       'gclid', 'isvideoad', 'page', 'slot', 'targetingcriteria', 'campaign',\n",
       "       'campaigncode', 'istruedirect', 'keyword', 'medium', 'referralpath',\n",
       "       'source', 'channelgrouping'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_df.columns[cat_feets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of training a set of trees and losing most of the trees, let's make an ensemble.  We can blend it and fit to the actual target with a single ensemble in two stages; we fit by session while still preventing user leakage before using these to predict the sum of all sessions and fitting on a separate validation set.\n",
    "\n",
    "If the number of folds is too large or small, we find that the system begins to more easily overfit and converges at a much lower score.  At the time of writing this, care was not taken in splitting the ids further, and many of these trees may be missing users who actually make purchases.  Too few trees and there is little information to be gained.  Somehow, the score is worse in this situation.  The code to execute the experiment is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 48.8659187\ttotal: 116ms\tremaining: 1m 55s\n",
      "250:\tlearn: 41.3518261\ttotal: 30.6s\tremaining: 1m 31s\n",
      "500:\tlearn: 38.2852292\ttotal: 1m 1s\tremaining: 1m 1s\n",
      "750:\tlearn: 36.1617326\ttotal: 1m 33s\tremaining: 31.2s\n",
      "999:\tlearn: 34.1333437\ttotal: 2m 4s\tremaining: 0us\n",
      "0:\tlearn: 40.3011697\ttotal: 56.6ms\tremaining: 56.5s\n",
      "250:\tlearn: 35.8040153\ttotal: 37.7s\tremaining: 1m 52s\n",
      "500:\tlearn: 33.3672442\ttotal: 1m 10s\tremaining: 1m 9s\n",
      "750:\tlearn: 31.4574017\ttotal: 1m 46s\tremaining: 35.3s\n",
      "999:\tlearn: 29.9630908\ttotal: 2m 30s\tremaining: 0us\n",
      "0:\tlearn: 42.8561723\ttotal: 101ms\tremaining: 1m 41s\n",
      "250:\tlearn: 34.6871193\ttotal: 38.7s\tremaining: 1m 55s\n",
      "500:\tlearn: 32.3960666\ttotal: 1m 17s\tremaining: 1m 16s\n",
      "750:\tlearn: 30.9622778\ttotal: 2m 11s\tremaining: 43.5s\n",
      "999:\tlearn: 29.9983139\ttotal: 3m 5s\tremaining: 0us\n",
      "0:\tlearn: 55.8675642\ttotal: 154ms\tremaining: 2m 33s\n",
      "250:\tlearn: 48.6006424\ttotal: 38.4s\tremaining: 1m 54s\n",
      "500:\tlearn: 45.3134388\ttotal: 1m 20s\tremaining: 1m 20s\n",
      "750:\tlearn: 43.0225738\ttotal: 2m 3s\tremaining: 40.9s\n",
      "999:\tlearn: 41.4429300\ttotal: 2m 42s\tremaining: 0us\n",
      "0:\tlearn: 44.9633519\ttotal: 172ms\tremaining: 2m 52s\n",
      "250:\tlearn: 35.8293629\ttotal: 41.2s\tremaining: 2m 2s\n",
      "500:\tlearn: 33.4297629\ttotal: 1m 24s\tremaining: 1m 24s\n",
      "750:\tlearn: 32.1798592\ttotal: 2m 8s\tremaining: 42.8s\n",
      "999:\tlearn: 31.1935232\ttotal: 2m 52s\tremaining: 0us\n",
      "0:\tlearn: 44.5168630\ttotal: 171ms\tremaining: 2m 51s\n",
      "250:\tlearn: 37.9752395\ttotal: 44.2s\tremaining: 2m 11s\n",
      "500:\tlearn: 35.4535071\ttotal: 1m 26s\tremaining: 1m 26s\n",
      "750:\tlearn: 33.5680942\ttotal: 2m 10s\tremaining: 43.2s\n",
      "999:\tlearn: 31.9432543\ttotal: 2m 51s\tremaining: 0us\n",
      "0:\tlearn: 47.2773201\ttotal: 165ms\tremaining: 2m 45s\n",
      "250:\tlearn: 41.4643981\ttotal: 41.6s\tremaining: 2m 4s\n",
      "500:\tlearn: 39.4625713\ttotal: 1m 24s\tremaining: 1m 23s\n",
      "750:\tlearn: 37.9193304\ttotal: 2m 5s\tremaining: 41.7s\n",
      "999:\tlearn: 36.6273563\ttotal: 2m 47s\tremaining: 0us\n",
      "0:\tlearn: 33.3770282\ttotal: 139ms\tremaining: 2m 18s\n",
      "250:\tlearn: 26.7371744\ttotal: 39.1s\tremaining: 1m 56s\n",
      "500:\tlearn: 24.8677331\ttotal: 1m 18s\tremaining: 1m 18s\n",
      "750:\tlearn: 23.4065384\ttotal: 1m 59s\tremaining: 39.6s\n",
      "999:\tlearn: 22.2443578\ttotal: 2m 39s\tremaining: 0us\n",
      "0:\tlearn: 38.1188529\ttotal: 147ms\tremaining: 2m 26s\n",
      "250:\tlearn: 30.7518608\ttotal: 39s\tremaining: 1m 56s\n",
      "500:\tlearn: 27.3705017\ttotal: 1m 12s\tremaining: 1m 12s\n",
      "750:\tlearn: 25.6629431\ttotal: 1m 44s\tremaining: 34.6s\n",
      "999:\tlearn: 24.7077131\ttotal: 2m 18s\tremaining: 0us\n",
      "0:\tlearn: 67.8146231\ttotal: 181ms\tremaining: 3m\n",
      "250:\tlearn: 63.9001404\ttotal: 42.1s\tremaining: 2m 5s\n",
      "500:\tlearn: 61.6411518\ttotal: 1m 25s\tremaining: 1m 24s\n",
      "750:\tlearn: 59.4789211\ttotal: 2m 8s\tremaining: 42.6s\n",
      "999:\tlearn: 56.7904462\ttotal: 2m 57s\tremaining: 0us\n",
      "0:\tlearn: 33.5868924\ttotal: 109ms\tremaining: 1m 48s\n",
      "250:\tlearn: 28.9055401\ttotal: 50.9s\tremaining: 2m 31s\n",
      "500:\tlearn: 27.2137343\ttotal: 1m 46s\tremaining: 1m 45s\n",
      "750:\tlearn: 25.8223907\ttotal: 2m 24s\tremaining: 48s\n",
      "999:\tlearn: 24.9228425\ttotal: 2m 56s\tremaining: 0us\n",
      "0:\tlearn: 137.3908582\ttotal: 121ms\tremaining: 2m 1s\n",
      "250:\tlearn: 110.7423744\ttotal: 33.9s\tremaining: 1m 41s\n",
      "500:\tlearn: 95.1667027\ttotal: 1m 7s\tremaining: 1m 7s\n",
      "750:\tlearn: 84.2239557\ttotal: 1m 41s\tremaining: 33.6s\n",
      "999:\tlearn: 75.3641837\ttotal: 2m 14s\tremaining: 0us\n",
      "0:\tlearn: 42.1710993\ttotal: 148ms\tremaining: 2m 28s\n",
      "250:\tlearn: 35.5468731\ttotal: 29.9s\tremaining: 1m 29s\n",
      "500:\tlearn: 32.5564167\ttotal: 1m 1s\tremaining: 1m 1s\n",
      "750:\tlearn: 30.1053979\ttotal: 1m 31s\tremaining: 30.4s\n",
      "999:\tlearn: 28.2600546\ttotal: 2m 2s\tremaining: 0us\n",
      "0:\tlearn: 49.0952290\ttotal: 135ms\tremaining: 2m 15s\n",
      "250:\tlearn: 40.1592537\ttotal: 31.5s\tremaining: 1m 33s\n",
      "500:\tlearn: 37.1095359\ttotal: 1m 3s\tremaining: 1m 3s\n",
      "750:\tlearn: 34.9840275\ttotal: 1m 35s\tremaining: 31.6s\n",
      "999:\tlearn: 33.2916781\ttotal: 2m 6s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "id_holder = train_ids\n",
    "fold = 20\n",
    "holdouts = 5\n",
    "fold_size = len(id_holder)//fold\n",
    "\n",
    "models = []\n",
    "first_ids = id_holder[:fold_size].copy()\n",
    "\n",
    "\n",
    "def trainCB(train_set, eval_pool, model_list):\n",
    "    model_list.append(cb.CatBoostRegressor(iterations = 1000,\n",
    "                                           learning_rate  = .1,\n",
    "                                           l2_leaf_reg = 100,\n",
    "                                           cat_features = cat_feets,\n",
    "                                           verbose = True))\n",
    "    model_list[-1].fit(X = train_set.drop('transactionrevenue', axis=1),\n",
    "                       y = train_set.transactionrevenue,\n",
    "                       #use_best_model = True, \n",
    "                       #eval_set = eval_pool, \n",
    "                       #early_stopping_rounds = 10, \n",
    "                       metric_period = 250)\n",
    "\n",
    "eval_chunk, id_holder = createChunk(id_holder, fold_size)\n",
    "eval_chunk = cb.Pool(eval_chunk.drop('transactionrevenue', axis=1), \n",
    "                 eval_chunk.transactionrevenue, \n",
    "                 cat_features=cat_feets)\n",
    "chunk, id_holder = createChunk(id_holder, fold_size)\n",
    "trainCB(chunk, eval_chunk, models)\n",
    "\n",
    "\n",
    "for n in range(fold - holdouts - 2):\n",
    "    eval_chunk = chunk\n",
    "    eval_chunk = cb.Pool(eval_chunk.drop('transactionrevenue', axis=1), \n",
    "                     eval_chunk.transactionrevenue, \n",
    "                     cat_features=cat_feets)\n",
    "    if n < (fold - holdouts - 3):\n",
    "        chunk, id_holder = createChunk(id_holder, fold_size)\n",
    "    else:\n",
    "        chunk = createChunk(first_ids, fold_size)[0]\n",
    "    trainCB(chunk, eval_chunk, models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the arbitrary fold size of 8, we still get over sixty-two thousand users, and expect a good number of users to be paying customers.  We reserve 3 of these folds for validation and predicting the log of the sums.  We will also use the previous fold for validation, mostly to save space and time to be honest, though the first and last rounds must be treated separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a L2 coefficient of 100, trees tend to stop around 100 iterations.  At a coefficient of 1, they stop $O(10)$ iterations, with the RMSE still increasing only 1 unit by the end.  At 1000, the training runs for more iterations, but the gains are just as modest.  There is still the question of whether it is better to let the ensemble components overfit, but we might run into the problem we usually do with random forests.  At this rate, though, the system will do not much better than a small collection of independent decision trees.  We'll go ahead and try anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del chunk, eval_chunk"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('../models/cb_overfit_stage1.pkl', 'wb') as f:\n",
    "    pickle.dump(models, f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('../models/cb_overfit_stage1.pkl', 'rb') as f:\n",
    "    models = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = createChunk(id_holder, len(id_holder), drop_ids=False, ignore_bounce=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x = val_df.drop('transactionrevenue', axis=1)\n",
    "val_y = val_df[['fullvisitorid', 'transactionrevenue']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could inject zeros here, but catboost doesn't like predicting on series objects for some reason, and transforming them into dataframes and transposing those seems to take longer I think.  Should time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = [model.predict(val_x) for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(models)):\n",
    "    val_x[f'predicted_{i}'] = (1 - val_x.bounces)*predicts[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_columns = ['fullvisitorid', 'pageviews', 'newvisits', 'visitnumber', 'bounces', 'sincelast', 'sincetwo', 'pageviewslast', 'pageviewstwo']\n",
    "col_mask = [column in kept_columns or 'predicted_' in column for column in val_x.columns.values]\n",
    "x = val_x.loc[:,col_mask].groupby('fullvisitorid').sum()\n",
    "x2 = val_x.loc[:,col_mask].groupby('fullvisitorid').std()\n",
    "x = x.join(x2, lsuffix='_mean', rsuffix='_std')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just realized some of these will be negative.  We need to transform them with a function that runs from zero to infinity and accepts negative infinity to infinity, or else use a different model.  We try the former here.  Why not, headed down a bad way at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = val_y.groupby('fullvisitorid').sum().applymap(lambda x: np.log(10**6*x + 1)).loc[x.index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(len(x)*.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = cb.Pool(data = x.iloc[:split, :], label = y.iloc[:split, :])\n",
    "train_set = cb.Pool(data = x.iloc[split:, :], label = y.iloc[split:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "censemble = cb.CatBoostRegressor(iterations = 3000,\n",
    "                                 learning_rate  = .03,\n",
    "                                 l2_leaf_reg = 100,\n",
    "                                 use_best_model = True,\n",
    "                                 bagging_temperature = 1,\n",
    "                                 depth = 5,\n",
    "                                 verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric iscalculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 2.0887349\ttest: 2.0886430\tbest: 2.0886430 (0)\ttotal: 31.8ms\tremaining: 1m 35s\n",
      "20:\tlearn: 1.8329968\ttest: 1.8333747\tbest: 1.8333747 (20)\ttotal: 438ms\tremaining: 1m 2s\n",
      "40:\tlearn: 1.7214156\ttest: 1.7254425\tbest: 1.7254425 (40)\ttotal: 822ms\tremaining: 59.3s\n",
      "60:\tlearn: 1.6724538\ttest: 1.6807761\tbest: 1.6807761 (60)\ttotal: 1.2s\tremaining: 57.8s\n",
      "80:\tlearn: 1.6490006\ttest: 1.6607681\tbest: 1.6607681 (80)\ttotal: 1.59s\tremaining: 57.4s\n",
      "100:\tlearn: 1.6363627\ttest: 1.6520609\tbest: 1.6520609 (100)\ttotal: 2.01s\tremaining: 57.6s\n",
      "120:\tlearn: 1.6283714\ttest: 1.6472127\tbest: 1.6472127 (120)\ttotal: 2.41s\tremaining: 57.4s\n",
      "140:\tlearn: 1.6228607\ttest: 1.6449618\tbest: 1.6449618 (140)\ttotal: 2.79s\tremaining: 56.6s\n",
      "160:\tlearn: 1.6183234\ttest: 1.6434167\tbest: 1.6434167 (160)\ttotal: 3.16s\tremaining: 55.7s\n",
      "180:\tlearn: 1.6148967\ttest: 1.6426458\tbest: 1.6426458 (180)\ttotal: 3.53s\tremaining: 55s\n",
      "200:\tlearn: 1.6122224\ttest: 1.6424368\tbest: 1.6424368 (200)\ttotal: 3.93s\tremaining: 54.8s\n",
      "220:\tlearn: 1.6093451\ttest: 1.6420109\tbest: 1.6419061 (218)\ttotal: 4.3s\tremaining: 54.1s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 1.641906104\n",
      "bestIteration = 218\n",
      "\n",
      "Shrink model to first 219 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x1a433e85f8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "censemble.fit(train_set, eval_set=eval_set, use_best_model=True, early_stopping_rounds = 10, metric_period = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    935\u001b[0m         keys, values, mutated = self.grouper.apply(f, self._selected_obj,\n\u001b[0;32m--> 936\u001b[0;31m                                                    self.axis)\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m   2272\u001b[0m             \u001b[0mgroup_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2274\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_indexed_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mcurried_with_axis\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mcurried_with_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_with_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: diff() got an unexpected keyword argument 'axis'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurried_with_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3468\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3469\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSeriesGroupBy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    929\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0m_group_selection_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    935\u001b[0m         keys, values, mutated = self.grouper.apply(f, self._selected_obj,\n\u001b[0;32m--> 936\u001b[0;31m                                                    self.axis)\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m   2272\u001b[0m             \u001b[0mgroup_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2274\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_indexed_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mcurried_with_axis\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mcurried_with_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_with_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: diff() got an unexpected keyword argument 'axis'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-53127fa51d72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateChunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_bounce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-13149835c2a5>\u001b[0m in \u001b[0;36mcreateChunk\u001b[0;34m(ids_list, size, drop_ids, ignore_bounce)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msomeppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msomeppl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetUserData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msomeppl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madjustCols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_bounce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-f7a0b6c6c5bf>\u001b[0m in \u001b[0;36madjustCols\u001b[0;34m(df, drop_ids, ignore_bounce)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'{col}last'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fullvisitorid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'{col}two'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fullvisitorid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sincelast'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fullvisitorid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisitstarttime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdays\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m86400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sincetwo'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fullvisitorid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisitstarttime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdays\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m86400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hour'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisitstarttime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhour\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    836\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    839\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3467\u001b[0m                       examples=_apply_docs['series_examples']))\n\u001b[1;32m   3468\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3469\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSeriesGroupBy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3471\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_agg_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0moption_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mode.chained_assignment'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         keys, values, mutated = self.grouper.apply(f, self._selected_obj,\n\u001b[0;32m--> 936\u001b[0;31m                                                    self.axis)\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m         return self._wrap_applied_output(\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m   2271\u001b[0m             \u001b[0;31m# group might be modified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2272\u001b[0m             \u001b[0mgroup_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2274\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_indexed_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m                 \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mcurried\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mcurried\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m             \u001b[0;31m# preserve the name so we can detect it when calling plot methods,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mdiff\u001b[0;34m(self, periods)\u001b[0m\n\u001b[1;32m   1991\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1992\u001b[0m         \"\"\"\n\u001b[0;32m-> 1993\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1994\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mdiff\u001b[0;34m(arr, n, axis)\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_timedelta\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1823\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimedeltaIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1824\u001b[0;31m         out_arr = TimedeltaIndex(out_arr.ravel().astype('int64')).asi8.reshape(\n\u001b[0m\u001b[1;32m   1825\u001b[0m             out_arr.shape).astype('timedelta64[ns]')\n\u001b[1;32m   1826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/timedeltas.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, unit, freq, start, end, periods, closed, dtype, copy, name, verify_integrity)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 name=None, verify_integrity=True):\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimedeltaIndex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_df = createChunk(test_ids, len(test_ids), drop_ids=False, ignore_bounce=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test_df.drop('transactionrevenue', axis=1)\n",
    "test_y = test_df[['fullvisitorid', 'transactionrevenue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(models)):\n",
    "    test_x[f'predicted_{i}'] = (1 - test_x.bounces)*models[i].predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test_x.loc[:,col_mask].groupby('fullvisitorid').sum()\n",
    "x2 = test_x.loc[:,col_mask].groupby('fullvisitorid').std()\n",
    "#x3 = val_x.loc[:,col_mask].groupby('fullvisitorid').max()\n",
    "x = x.join(x2, lsuffix='_mean', rsuffix='_std')\n",
    "#x = x.join(x3, rsuffix='_max')\n",
    "\n",
    "y = test_y.groupby('fullvisitorid').sum().applymap(lambda x: np.log(10**6*x + 1)).loc[x.index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "censemble.score(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(censemble.predict(x), y)\n",
    "plt.plot((0,25), (0,25), linestyle='--', color='k')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('../models/cb_overfit_stage2.pkl', 'wb') as f:\n",
    "    pickle.dump(censemble, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
